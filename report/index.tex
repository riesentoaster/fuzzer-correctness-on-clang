\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\usepackage{listings}
\usepackage{hyperref}

\lstset{
  basicstyle=\ttfamily\footnotesize,
  frame=single,
}



\let\savedCite=\cite
\renewcommand{\cite}{\unskip~\savedCite}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2026}
\acmYear{2026}
\setcopyright{cc}
\setcctype{by}
\acmConference[ICSE-Companion '26]{2026 IEEE/ACM 48th International Conference on Software Engineering}{April 12--18, 2026}{Rio de Janeiro, Brazil}
\acmBooktitle{2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE-Companion '26), April 12--18, 2026, Rio de Janeiro, Brazil}
% \acmPrice{}
\acmDOI{10.1145/3774748.3787671}
\acmISBN{979-8-4007-2296-7/2026/04}




\begin{document}

\title{Testing Programs Expecting Highly Constrained Inputs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Valentin Huber}
\email{valentin.huber@cispa.de}
\orcid{0009-0002-9869-4583}
\affiliation{%
    \institution{CISPA Helmholtz Center for Information Security}
    \city{Saarbrücken}
    \country{Germany}
    % \\ Year 1 out of expected 4
    % \\ Supervised by Prof. Dr. Andreas Zeller
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    Current test generators are limited in the inputs they produce. This leads to an imbalance in the program parts tested, with significant program parts receiving insufficient attention, particularly with programs expecting inputs that are highly structured. In this work, I propose two measurements to evaluate different approaches and find which parts of programs are under-tested. \textit{Input correctness} provides a level to which an input satisfies the expected structure, measured either against an implementation or specification. \textit{Input interestingness} checks the distance to equivalence class boundaries.

    I present the distribution of \textit{input correctness} from an initial evaluation on a \texttt{clang}, indicating that approaches designed to provide structurally valid inputs are unable to test program logic behind the input validation steps. I finally propose three approaches to closing this gap by producing inputs that are both \textit{correct} and \textit{interesting}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
    <ccs2012>
    <concept>
    <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
    <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
    <concept>
    <concept_id>10003752.10003766.10003771</concept_id>
    <concept_desc>Theory of computation~Grammars and context-free languages</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
    <concept>
    <concept_id>10011007.10010940.10010992.10010998.10011001</concept_id>
    <concept_desc>Software and its engineering~Dynamic analysis</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
    </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[300]{Theory of computation~Grammars and context-free languages}
\ccsdesc[300]{Software and its engineering~Dynamic analysis}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Automated Testing, Grammars, Test Generator Evaluation}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\subsubsection*{Doctoral Symposium Information}
I am in year 1 out of expected 4 in my PhD, supervised by Prof. Dr. Andreas Zeller.


\begin{figure}
    \begin{lstlisting}
        (P1): VBq-"7.6arK w;zu%6$K>%OTV"ryD*
    \end{lstlisting}
    \begin{lstlisting}
        (P2): && float += % " ( ; == [
        \end{lstlisting}
    \begin{lstlisting}
            (P3): int main() { return i; }
        \end{lstlisting}
    \begin{lstlisting}
            (P4): int main() { return 0; }
        \end{lstlisting}
    \caption{C programs that are lexically \texttt{(P1)}, syntactically \texttt{(P2)} and semantically \texttt{(P3)} invalid and fully valid \texttt{(P4)}.}
    \Description{Examples of C programs that are lexically, syntactically and semantically invalid and fully valid.}
    \label{fig:c_examples}
\end{figure}
\section{Introduction}
\label{sec:introduction}

Programs processing input typically do so in distinct steps, with the programs in Figure~\ref{fig:c_examples} passing increasingly more of these:

\begin{enumerate}
    \item \textbf{Lexing}: Unstructured input is split into an unstructured list of tokens. \texttt{(P1)} will be rejected by the lexer of a C compiler.
    \item \textbf{Syntactic Parsing}: If the input is syntactically valid, it can be parsed into structures. While \texttt{(P2)} consists of exclusively valid tokens, they do not appear in the correct structure.
    \item \textbf{Semantic Checks}: The parsed structures are then subject to tests on their semantics. \texttt{(P3)} for example can be parsed, but is invalid because the variable \texttt{i} is not defined.
    \item \textbf{Business Logic}: Finally, the input is processed by a program's inner logic, e.g. translated into an executable.
\end{enumerate}

\begin{figure}[b]
    \includegraphics[width=0.77\linewidth]{bush}
    \caption{Bush representing the input space of a program, with increasing height representing increasing input correctness, assessed by the input processing steps.}
    \Description{Drawing of a bush with a ladder and overlayed numbered ellipses. The bush represents the input space of a system under test, with increasing height in the bush representing increasing input correctness, assessed by the input processing steps.}
    \label{fig:bush}
\end{figure}

My observation is that test generators generally produce inputs that test only a subset of the steps above, depending on their internal model of the input structure. Some of my previous work\cite{FTZ,coreutils} has explored this by building purpose-built fuzzers that ensure (partial) correctness of inputs.

\subsection{Categorization of Existing Approaches}

Imagine the input space of a program as a bush, with bugs represented by berries spread throughout, as shown in Figure~\ref{fig:bush}.

Purely \textit{random input generation}\cite{UNIX} is unlikely to test anything but the lexing stage; it is therefore similar to reaching only the very bottom of the bush (I). In a next step, \textit{coverage-guided fuzzing} was proposed, which is able to incrementally find inputs that reach deeper into the program with random mutation\cite{AFL} and thus reaches further up the bush (II). These approaches were extended with \textit{additional instrumentation and logic} that allow them to produce increasingly correct inputs\cite{RedQueen,FrameShift}, thus extending their reach once more (III). By manually providing the test generator with a model of the target input structure, an analyst can help a test generator produce increasingly correct inputs. This is the equivalent of helping a test generator climb on the first step of a ladder.

One widely used approach is to provide a test generator with a \textit{context-free grammar}, with which it can produce inputs that will, by construction, pass the parser and thus reach further into the program (IV). From there, they can produce inputs that \textit{violate} their model\cite{GrammarMutation,Nautilus} to reach further down (V), or attempt to \textit{pass additional semantic checks}\cite{Autarkie} (VI). Recent advancements provide a generic model for input testers to receive a \textit{complete model} of the PUT input structure\cite{Fandango}, which allows testing the business logic (VII).

Test generators based on \textit{symbolic executions} systematically attempt to cover all branches in the program and bush, but has significant limitations, as discussed in some of my prior work\cite{SymbexSurvey}.

To the best of my knowledge, there is no systematic evaluation of representatives of these approaches — we do not know the extent of their reach. Following this, we do not know what parts of targets remain untested, even when multiple approaches are combined.

Based on this insight, I present the following contributions:

\begin{enumerate}
    \item I propose two measurements based on which to compare the distribution of inputs from different test generators: input \textit{correctness}, and input \textit{interestingness} (Section~\ref{sec:measurements}).
    \item I provide initial experiments and their results, which suggest that existing input generators are limited in testing all program logic (Section~\ref{sec:experiments}).
    \item I propose three approaches to creating input generators that address these limitations (Section~\ref{sec:my_fuzzers}).
\end{enumerate}

\section{Evaluating Test Input Quality}
\label{sec:measurements}

Test generators can be evaluated along multiple, orthogonal axes. To evaluate their ability to reach all program parts, and focus on particularly interesting inputs, I propose two measurements, along with implementation approaches.

\subsection{Input Correctness}

To test a test generator's ability to reach all program parts, I propose evaluating them for \textit{input correctness}. This can be defined and measured in two ways:

\begin{enumerate}
    \item First, one can test inputs \textbf{against an implementation} of a PUT, measuring the ratio of accepted to rejected inputs at each stage of the input processing pipeline. This can be achieved with one of the following:
          \begin{enumerate}
              \item Manual annotations of steps, similar to \cite{FuzzFactory,IJON}.
              \item Based on a diverse set of seed inputs, count edges of generated inputs that are executed by \textit{all}, or \textit{any} valid input.
          \end{enumerate}
    \item Alternatively, inputs can be evaluated \textbf{against an abstract specification} of the language expected by a PUT. Such specifications are usually given in natural language. Recent works have explored how to express such specifications in a structured way, as a combination of a context-free grammar and additional constraints over nodes in this grammar\cite{Fandango}. Evaluating the correctness against specification may be done in different steps:
          \begin{enumerate}
              \item For inputs that cannot be parsed, existing literature provides algorithms to measure inputs' to the grammar\cite{DistanceLessThanCubic}.
              \item For grammar-valid inputs, the ratio of fulfilled to violated constraints is used.
          \end{enumerate}
\end{enumerate}

\subsection{Input Interestingness}

Not all inputs are equally interesting, even if they are all fully correct or are processed by the exact same instructions. Assume a constraint checking if a person is over 18. According to the principle of equivalence classes, mutating their age to random values, say from $27$ to $43$, is unlikely to trigger a new bug. Changing it to values near the boundaries of equivalence classes, in this case $18$ or $19$, is a more promising strategy to discover edge case bugs, such as off-by-one errors. Based on this insight, I define \textit{Input interestingness} as the distance of an input to equivalence class boundaries.

Previous work proposed mutating input parts to contain generally interesting values, such as $0$, $-1$, or $2^{16}$\cite{AFL}. However, the lack of approaches to specify input correctness fully meant that evaluating distance of an input to equivalence class boundaries was limited by an analyst's ability to manually annotate select boundaries\cite{IJON}. With the advent of abstract full input specifications\cite{Fandango}, one can now test this automatically:

\begin{enumerate}
    \item \textbf{Semantic Interestingness}: Inputs are more interesting if they are closer to boundaries of constraints, as described in the example above.
    \item \textbf{Syntactic Boundaries}: If an input part can be repeated an arbitrary number of times, repeating it 0, 1, or 999999 times is more interesting.
\end{enumerate}

\begin{table}
    \centering
    \caption{Ratio of inputs rejected by program steps, total coverage, and ratio of approximated duplicates across approach categories in clang. Approaches marked with \textbf{*} receive an initial corpus of valid C programs.}
    \label{fig:results}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{l|ccccccc}
                           & \textbf{Other} & \textbf{Lexing} & \textbf{Syntax} & \textbf{Semantic} & \textbf{Valid} & \textbf{Cov} & \textbf{Duplicate} \\\hline
            % \textbf{*}     & 0.000\%        & 0.000\%         & 0.000\%         & 0.000\%           & 1.0000\%       & 12772        & 0                  \\
            \textbf{(II)}  & 0.001\%        & 0.566\%         & 0.362\%         & 0.066\%           & 0.0048\%       & 12920        & 11.194\%           \\
            \textbf{(II)*} & 0.000\%        & 0.357\%         & 0.317\%         & 0.316\%           & 0.0101\%       & 12917        & 0.886\%            \\
            \textbf{(IV)}  & 0.054\%        & 0.293\%         & 0.164\%         & 0.489\%           & 0.0006\%       & 13075        & 0.673\%            \\
            \textbf{(IV)*} & 0.054\%        & 0.292\%         & 0.164\%         & 0.489\%           & 0.0007\%       & 13072        & 0.650\%            \\
            \textbf{(V)}   & 0.008\%        & 0.397\%         & 0.231\%         & 0.364\%           & 0.0010\%       & 12915        & 1.563\%            \\
            \textbf{(V)*}  & 0.008\%        & 0.396\%         & 0.230\%         & 0.365\%           & 0.0012\%       & 13077        & 1.592\%            \\
            \textbf{(VI)}  & 0.000\%        & 0.000\%         & 0.571\%         & 0.429\%           & 0.0000\%       & 12793        & 14.672\%           \\
            \textbf{(VI)*} & 0.000\%        & 0.003\%         & 0.553\%         & 0.444\%           & 0.0000\%       & 12920        & 14.989\%           \\
        \end{tabular}
    }
\end{table}

\section{Initial Experiments and Results}
\label{sec:experiments}

Section~\ref{sec:introduction} introduces potential limitations of existing input generators. To evaluate this claim, I test representatives from a subset of the proposed categories against the C compiler \texttt{clang}. I evaluate the different approaches for \textit{input correctness}, as measured through manual annotation of additional instrumentation in the target.

Table~\ref{fig:results} presents the ratios of inputs generated by the evaluated approaches that reach a certain step during program execution. Approach (II) is built using a binary-mutational coverage-guided fuzzer. Test generator (IV) is represented by a pure grammar-based test generator. For (V), outputs from the grammar test generator are used, both unchanged and binary mutated. (VI) is represented by a coverage-guided grammar test generator. All representatives were run twice, once with only self-created seeds, and once with an additional set of 10 diverse, manually written, high-quality seeds.

The results confirm the suspected inability of the presented approaches to test program parts past the semantic checks in significant numbers. And while fully duplicate inputs are probabilistically filtered using a bloom filter, we do not filter semantically equivalent and potentially trivial programs, such as empty programs with a repeatedly mutated comment. Notably (branch) coverage data does not seem to show a significant correlation to the approach or error step distribution. This suggests that coverage is an imprecise measure of both semantic target coverage and input (and thus testing) distribution across target parts.

These results however do have significant limitations and only partially adhere to general evaluation best practices\cite{SOKFuzzingEvaluation}. I am only testing against one implementation for one target, which requires highly structured inputs. I am evaluating only on \textit{input correctness}, in coarse-grained steps, and not against a specification. Finally, the examples were only run once, for 12 hours each.

In future work, I want to expand on these experiments by extending the number and kind of targets, test generator approaches and implementations, measurements — all in a more robust evaluation.

\section{Reaching the Remaining Bush}
\label{sec:my_fuzzers}

The results in Section~\ref{sec:experiments} suggest that current approaches for test generators are limited — large parts of \texttt{clang} remain untested. Section~\ref{sec:introduction} provides a conceptual explanation for these limitations. Building on top of these, I present three approaches that extend the parts of a PUT that are effectively tested by test generators:
\begin{enumerate}
    \item \textbf{Reach down from the top}: Similarly to out-of-grammar test generation, I want to explore out-of-language generation, where constraints of a full specification are iteratively and deliberately violated while the remaining constraints are still fulfilled. This approach could further be improved by out-of-grammar construction of the inputs that are evaluated for constraints, giving the test generator theoretical reach from the top of the ladder, all the way to the ground.
    \item \textbf{Incremental steps towards the top}: Writing fully correct specifications for tools like Fandango\cite{Fandango} requires considerable effort. However, extending an existing grammar with a few simple, yet incomplete, constraints may be enough to allow a test generator to reach code it was previously unable to test. This will still produce inputs not matching the full specification, thus leading to similar results as approach (1).
    \item \textbf{Targeted testing}: Due to the availability of tools able to use full input specifications, I would like to explore test generators producing inputs that reach higher interestingness in their test cases by steering input generation towards boundaries of both the input semantics (i.e., the constraints) and the syntax (the structure).
\end{enumerate}

\section{Conclusion}

In this work, I present two measurements to evaluate different test generation approaches. \textit{Input correctness} measures either how far an input gets along the input processing pipeline of a target program, or to what extent it fulfills analyst-given lexical, syntactic, and semantic constraints. \textit{Input interestingness} refers to the distance between an input and equivalence class boundaries.

I then present results of an initial experiment showing that current generic test generators fail at testing significant parts of a C compiler, as measured by \textit{input correctness}. Finally, I propose three approaches to creating test generators that are able to test previously untestable program parts. To achieve improved \textit{input correctness}, systematically violate semantic and syntactic rules of an input specification, or extend current, structure-aware test generators based on context-free grammars with additional, incomplete semantic constraints. To produce inputs with increased \textit{input interestingness}, prioritize inputs closer to equivalence class boundaries.

The experiment setup and detailed results can be found at\\\href{https://github.com/riesentoaster/fuzzer-correctness-on-clang}{github.com/riesentoaster/fuzzer-correctness-on-clang}.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
    This work is funded by the European Union (ERC S3, 101093186). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sources.bib}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.